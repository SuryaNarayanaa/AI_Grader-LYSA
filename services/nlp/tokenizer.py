import re
import string

def tokenize(text):
    # Tokenize the text   
    return 

def remove_stopwords(tokens, stopwords=None):
    return

def stem_tokens(tokens):
    
    # Stemming the tokens
    return 